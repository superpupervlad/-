{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Laba.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtyGsN41cJfW"
      },
      "source": [
        "# Введение\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUax1Iu4-uMR"
      },
      "source": [
        "Сегодня мы продолжим изучение применения методов анализа данных и машинного обучения на практических примерах. В прошлой ЛР мы с вами разбирались с задачей кластеризации. Теперь вас ждет новая работа - «Задачи о паспортах» (Задание №2).\n",
        "При решении будут показаны основы анализа текстовой информации, а также ее кодирование для построения модели с помощью Python и модулей для анализа данных (pandas, scikit-learn, pymorphy)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvhsHcjBBodz"
      },
      "source": [
        "Документация\n",
        "* [pandas](https://pandas.pydata.org/pandas-docs/stable/)\n",
        "* [scikit-learn](https://scikit-learn.org/stable/)\n",
        "* [pymorphy](https://pymorphy2.readthedocs.io/en/latest/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnA_utIej95A"
      },
      "source": [
        "Постановка задачи: \n",
        "\n",
        "При работе с большим объёмом данных важно поддерживать их чистоту. А при заполнении заявки на банковский продукт необходимо указывать полные паспортные данные, в том числе и поле «кем выдан паспорт», число различных вариантов написаний одного и того же отделения потенциальными клиентами может достигать нескольких сотен. Важно понимать, не ошибся ли клиент, заполняя другие поля: «код подразделения», «серию/номер паспорта». Для этого необходимо сверять «код подразделения» и «кем выдан паспорт».\n",
        "Задача заключается в том, чтобы проставить коды подразделений для записей из тестовой выборки, основываясь на обучающей выборке."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVscrtB571BU"
      },
      "source": [
        " Загрузим данные и все библиотеки, которые нам пригодятся. \n",
        " \n",
        " Для полного понимания и дополнительной информации вы можете обращаться к документации\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDarpaS_8HJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e68b808-2e1a-494e-b09a-40e9c7b6e4c8"
      },
      "source": [
        "pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\r\u001b[K     |██████                          | 10kB 16.2MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20kB 21.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30kB 11.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 6.0MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqQj2wqyqxna"
      },
      "source": [
        "from pandas import read_csv\n",
        "import pymorphy2\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY5bEcRi70dB"
      },
      "source": [
        "Давайте посмотрим, как выглядят наши \n",
        "\n",
        "---\n",
        "\n",
        "данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWqDtCRr80SJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59ac417-92e8-41a2-fa21-de91465a9ad6"
      },
      "source": [
        "train = read_csv('passport_training_set.csv',';', index_col='id' ,encoding='cp1251')\n",
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(96750, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34KOgIhp81Kb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "d3aa661b-6637-49fe-961a-5e352a74a324"
      },
      "source": [
        "train.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>passport_div_code</th>\n",
              "      <th>passport_issuer_name</th>\n",
              "      <th>passport_issue_month/year</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>422008</td>\n",
              "      <td>БЕЛОВСКИМ УВД КЕМЕРОВСКОЙ ОБЛАСТИ</td>\n",
              "      <td>11M2001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>500112</td>\n",
              "      <td>ТП №2 В ГОР. ОРЕХОВО-ЗУЕВО ОУФМС РОССИИ ПО МО ...</td>\n",
              "      <td>03M2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>642001</td>\n",
              "      <td>ВОЛЖСКИМ РОВД ГОР.САРАТОВА</td>\n",
              "      <td>04M2002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>162004</td>\n",
              "      <td>УВД МОСКОВСКОГО РАЙОНА Г.КАЗАНЬ</td>\n",
              "      <td>12M2002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>80001</td>\n",
              "      <td>ОТДЕЛОМ ОФМС РОССИИ ПО РЕСП КАЛМЫКИЯ В Г ЭЛИСТА</td>\n",
              "      <td>08M2009</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    passport_div_code  ... passport_issue_month/year\n",
              "id                     ...                          \n",
              "1              422008  ...                   11M2001\n",
              "2              500112  ...                   03M2009\n",
              "3              642001  ...                   04M2002\n",
              "4              162004  ...                   12M2002\n",
              "5               80001  ...                   08M2009\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40fXISlhpjmz"
      },
      "source": [
        "Мы с вами видим таблицу, состоящую из 3 колонок - Кода подразделения, Кем выдан паспорт и дата выдачи, в формате mm/yyyy. На данных из колонок passport_issuer_name и passport_issue_month/year мы будем тренировать наши данные. Данные из колонки passport_div_code - наш таргет. Именно код подразделения мы будем предсказывать.\n",
        "\n",
        "Т.е. на passport_issuer_name(Кем выдан) и passport_issue_month/year(Дата выдачи) мы должны тренировать наши данные, а предсказывать будем passport_div_code (Код подразделения)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnYVlWEbp4x_"
      },
      "source": [
        "Как вы можете заметить, данные в колонке passport_issuer_name представлены не в числовом, уже знакомым для нас, формате, а в текстовом. А это значит что нам предстоит привести текст в нормальную форму. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkvOqoRQ4h4h"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Предварительная обработка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqEu10D64p-5"
      },
      "source": [
        "Теперь можно посмотреть как пользователи записывают поле «кем выдан паспорт» на примере какого-либо подразделения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A0c_41j7UbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e68967fb-399d-4ba8-b009-2bf520a4660e"
      },
      "source": [
        "example_code = train.passport_div_code[train.passport_div_code.duplicated()].values[0]\n",
        "for i in train.passport_issuer_name[train.passport_div_code == example_code].drop_duplicates():\n",
        "    print (i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ОТДЕЛЕНИЕМ УФМС РОССИИ ПО РЕСПУБЛИКЕ КАРЕЛИЯ В МЕДВЕЖ. Р-Е\n",
            "ОТДЕЛЕНИЕМ УФМС РОССИИ ПО Р. КАРЕЛИЯ В МЕДВЕЖЬЕГОРСКОМ РАЙОНЕ\n",
            "ОТДЕЛЕНИЕМ УФМС РОССИИ ПО РЕСП КАРЕЛИЯ В МЕДВЕЖЬЕГОРСКОМ Р-НЕ\n",
            "ОТДЕЛЕНИЕМ УФМС РОССИИ ПО РЕСПУБЛИКЕ КАРЕЛИЯ В МЕДВЕЖЬЕГОРСКОМ РАЙОНЕ\n",
            "ОУФМС РОССИИ ПО РЕСПУБЛИКЕ КАРЕЛИЯ В МЕДВЕЖЬЕГОРСКОМ РАЙОНЕ\n",
            "УФМС РОССИИ ПО РК В МЕДВЕЖЬЕГОРСКОМ РАЙОНЕ\n",
            "ОТДЕЛЕНИЕМ УФМС РОССИИ ПО РЕСПУБЛИКЕ КАРЕЛИЯ МЕДВЕЖЬЕГОРСКОМ Р-ОНЕ\n",
            "ОТДЕЛЕНИЕМ УФМС РОССИИ ПО РК В МЕДВЕЖЬЕГОРСКОМ РАЙОНЕ\n",
            "ОТДЕЛЕНИЕМ УФМС РОССИИ ПО РЕСПУБЛИКЕ КОРЕЛИЯ В МЕДВЕЖИГОРСКОМ РАЙОНЕ\n",
            "УФМС РОССИИ ПО Р. КАРЕЛИЯ МЕДВЕЖЬЕГОРСКОГО Р-НА\n",
            "ОТДЕЛОМ УФМС РОССИИ ПО РЕСПУБЛИКЕ КАРЕЛИЯ В МЕДВЕЖЬЕГОРСКОМ\n",
            "УФМС РЕСПУБЛИКИ КАРЕЛИИ МЕДВЕЖЬЕГОРСКОГО Р-ОН\n",
            "МЕДВЕЖЬЕГОРСКИМ ОВД\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47AKgQLj4qDH"
      },
      "source": [
        "Как можно заметить нужное нам поле действительно заполняется криво. Но для нормально кодирования мы должны привести это поле к более-менее нормальному (однозначному) виду.\n",
        "Для начала вам предлагается привести все записи к одному регистру, например, чтобы все буквы стали строчными. Это легко сделать с помощью атрибута str, столбца DataFrame'a. Этот атрибут позволяет работать со столбцом как с строкой, а также выполнять различного рода поиск и замену по регулярным выражениям:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrLS_d3d_tBI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "dd344858-15a9-450c-c688-eb7066635854"
      },
      "source": [
        "train.passport_issuer_name = train.passport_issuer_name.str.lower()\n",
        "train[train.passport_div_code == example_code].head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>passport_div_code</th>\n",
              "      <th>passport_issuer_name</th>\n",
              "      <th>passport_issue_month/year</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>100010</td>\n",
              "      <td>отделением уфмс россии по республике карелия в...</td>\n",
              "      <td>04M2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>100010</td>\n",
              "      <td>отделением уфмс россии по р. карелия в медвежь...</td>\n",
              "      <td>10M2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5642</th>\n",
              "      <td>100010</td>\n",
              "      <td>отделением уфмс россии по респ карелия в медве...</td>\n",
              "      <td>08M2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6668</th>\n",
              "      <td>100010</td>\n",
              "      <td>отделением уфмс россии по республике карелия в...</td>\n",
              "      <td>08M2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8732</th>\n",
              "      <td>100010</td>\n",
              "      <td>отделением уфмс россии по республике карелия в...</td>\n",
              "      <td>08M2012</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      passport_div_code  ... passport_issue_month/year\n",
              "id                       ...                          \n",
              "19               100010  ...                   04M2008\n",
              "22               100010  ...                   10M2009\n",
              "5642             100010  ...                   08M2008\n",
              "6668             100010  ...                   08M2011\n",
              "8732             100010  ...                   08M2012\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb7Mmdf34qH4"
      },
      "source": [
        "C регистром определились. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7rDTL2s-jLa"
      },
      "source": [
        "####[Регулярные](https://tproger.ru/translations/regular-expression-python/) выражения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTTPCZOX1dYo"
      },
      "source": [
        "Говоря простым языком, регулярное выражение — это последовательность символов, используемая для поиска и замены текста в строке или файле.\n",
        "\n",
        "Регулярные выражения используют два типа символов:\n",
        "\n",
        "* специальные символы: как следует из названия, у этих символов есть специальные значения. Аналогично символу *, который как правило означает «любой символ» (но в регулярных выражениях работает немного иначе, о чем поговорим ниже);\n",
        "* литералы (например: a, b, 1, 2 и т. д.).\n",
        "\n",
        "В Python для работы с регулярными выражениями есть модуль re. Для использования его нужно импортировать:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TbNKQGr0J6r"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrFyR0Mo4XLq"
      },
      "source": [
        "Операторы и их описание\n",
        "\n",
        ".\tОдин любой символ, кроме новой строки \\n.\n",
        "\n",
        "\\w\tЛюбая цифра или буква (\\W — все, кроме буквы или цифры)\n",
        "\n",
        "\\d\tЛюбая цифра [0-9] (\\D — все, кроме цифры)\n",
        "\n",
        "\\s\tЛюбой пробельный символ (\\S — любой непробельный символ)\n",
        "\n",
        "[..]\tОдин из символов в скобках ([^..] — любой символ, кроме тех, что в \n",
        "скобках)\n",
        "\n",
        "\\\tЭкранирование специальных символов (\\. означает точку или \\+ — знак «плюс»)\n",
        "\n",
        "a|b\tСоответствует a или b\n",
        "\n",
        "()\tГруппирует выражение и возвращает найденный текст\n",
        "\n",
        "\\t, \\n, \\r\tСимвол табуляции, новой строки и возврата каретки соответственно"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuM_GSOP5V1a"
      },
      "source": [
        "Попробуйте с помощью функции re.findall() найти:\n",
        "\n",
        "* судьба, судоходство, судьбы, сударыня в тексте: \"Анатолий любил заниматься судоходством и верил, что это его судьба! Но ирония судьбы в том, что сударыня Екатерина так не думала\" ([а-я] - любая буква. [а-я]* - любые буквы дальше)\n",
        "* дворе, двора в тексте: \"На дворе трава, на траве дрова Не руби дрова на траве двора.\"\n",
        "* покупки, покупочки в тексте: \"Расскажите про покупки, Про какие про покупки? Про покупки, про покупки, Про покупочки мои.\"\n",
        "* чертили, чертенка, чертеж,: \"Четыре черненьких, чумазеньких чертенка Чертили черными чернилами чертеж.\" "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7tWNKHaBPPo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6055d0c8-d833-4804-fd8e-0141be55e94c"
      },
      "source": [
        "re.findall('(по\\куп[а-я]*|с\\куп[а-я]*)', 'Мы любим наших покупателей. Рады когда они покупают что-то. А еще у нас лучшие скупщики') #покупателей, покупают, скупщики"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['покупателей', 'покупают', 'скупщики']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKXgyEJ_5NTl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08bbff00-5cf1-479c-857a-af51278e14b9"
      },
      "source": [
        "re.findall('суд\\w*', 'Анатолий любил заниматься судоходством и верил, что это его судьба! Но ирония судьбы в том, что сударыня Екатерина так не думала') ## - ВАШ КОД ТУТ - ##"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['судоходством', 'судьба', 'судьбы', 'сударыня']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnYvOtYE9B1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22fdd0b-13f9-4223-e646-a58a93689c7a"
      },
      "source": [
        "re.findall('двор[е|а]', 'На дворе трава, на траве дрова Не руби дрова на траве двора.') ## - ВАШ КОД ТУТ - ##"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['дворе', 'двора']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mtfTv5x9B9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b3506f-bb3d-4802-d967-b49d0f7caa06"
      },
      "source": [
        "re.findall('(покуп[(ки)|(очки)])', 'Расскажите про покупки, Про какие про покупки? Про покупки, про покупки, Про покупочки мои.') ## - ВАШ КОД ТУТ - ##"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['покупк', 'покупк', 'покупк', 'покупк', 'покупо']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9FZZuYV9CBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836d7e4a-ef90-4040-d0bf-a4411e9f8037"
      },
      "source": [
        "re.findall('[Ч|ч]ерт\\w*', 'Четыре черненьких, чумазеньких чертенка Чертили черными чернилами чертеж.') ## - ВАШ КОД ТУТ - ##"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['чертенка', 'Чертили', 'чертеж']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBjLvVt7BSbx"
      },
      "source": [
        "##### Сокращения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl_qPivA-PJP"
      },
      "source": [
        "Далее надо по возможности избавиться от популярных сокращений, например район, город и т.д. Сделаем это с помощью регулярных выражений. Pandas предоставляет удобное использование регулярных выражений применительно к каждому столбцу. Это выглядит так: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVg800RH9Fx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d88b083a-f730-487a-d8a5-fec65e262951"
      },
      "source": [
        "result = re.search(r'р-(а|й|о|н|е)*', u'отделением уфмс россии по р. карелия в медвежь р-не') # Поиск в строке любых сокращений типа р- (а ИЛИ й ИЛИ о ИЛИ н ИЛИ е)\n",
        "result # в результате нам выдаст начало и конец того, что искали"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_sre.SRE_Match object; span=(47, 51), match='р-не'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqFAOnsMCp99"
      },
      "source": [
        "train.passport_issuer_name = train.passport_issuer_name.str.replace(u'р-(а|й|о|н|е)*',u'район ')\n",
        "train.passport_issuer_name = train.passport_issuer_name.str.replace(u' г( |\\.|(ор(\\.| )))', u' город ')\n",
        "train.passport_issuer_name = train.passport_issuer_name.str.replace(u' р(\\.|есп\\. )', u' республика ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB5ma3HcBZlA"
      },
      "source": [
        "Пропишите так же для сокращений административный(адм. админ. администр. итд), округ(окр., округа, окр), и административный округ(ао)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7U30wq4B6xU"
      },
      "source": [
        "train.passport_issuer_name = train.passport_issuer_name.str.replace(u' адм[ин\\.|инистр\\.|инистратив\\.] ',u' административный ')\n",
        "train.passport_issuer_name = train.passport_issuer_name.str.replace(u' окр[уг|уга|\\.] ',u'округ ')\n",
        "train.passport_issuer_name = train.passport_issuer_name.str.replace(u' ао ',u' административный округ ')\n",
        "## - ВАШ КОД ТУТ - ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z391g-gWBfGM"
      },
      "source": [
        "##### Символы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW5TwwCNEhL8"
      },
      "source": [
        "Теперь избавимся от всех лишних символов, кроме русских букв, дефисов и пробелов. Это связано с тем, что паспорт о одинаковым подразделением может выдаваться отделами с разными номерами, и это ухудшит дальнейшую кодировку:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZP6kf3QAy6O"
      },
      "source": [
        "train.passport_issuer_name = train.passport_issuer_name.str.replace(u' - ?', u'-')\n",
        "train.passport_issuer_name = train.passport_issuer_name.str.replace(u'[^а-я -]','')\n",
        "train.passport_issuer_name = train.passport_issuer_name.str.replace(u'- ',' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHmEmyaVCOQc"
      },
      "source": [
        "Избавтесь от пробелов так же"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4a3FKlHCNo-"
      },
      "source": [
        "#train.passport_issuer_name = train.passport_issuer_name.str.strip()\n",
        "train.passport_issuer_name = train.passport_issuer_name.str.replace(u'\\s+',' ')\n",
        "## - ВАШ КОД ТУТ - ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53yAUdR8BjKs"
      },
      "source": [
        "##### Аббревиатуры"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9Klqt07Aznw"
      },
      "source": [
        "На следующем шаге, надо расшифровать аббревиатуры, типа УВД, УФНС, ЦАО, ВАО и т.д., т.к. этих их в принципе не много, но на качестве дальнейшего кодирования это скажется положительно. Например если у нас будет две записи «УВД» и «управление внутренних дел», то закодированы они будут по разному, т. к. для компьютера это разные значения.\n",
        "Итак перейдем к расшифровке. И, для начала, заведем словарь сокращений, с помощью которого мы и сделаем расшифровку:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2tnK1-qCyMc"
      },
      "source": [
        "Добавьте в этот словарик еще аббревиатуры юао(южный), юзао(юго-западный), ювао(юго-восточный), пс(паспортный стол), тп(территориальный пункт)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MCxog9QAzty"
      },
      "source": [
        "sokr = {u'нао': u'ненецкий автономный округ',\n",
        "u'хмао': u'ханты-мансийский автономный округ',\n",
        "u'чао': u'чукотский автономный округ',\n",
        "u'янао': u'ямало-ненецкий автономный округ',\n",
        "u'вао': u'восточный административный округ',\n",
        "u'цао': u'центральный административный округ',\n",
        "u'зао': u'западный административный округ',\n",
        "u'cао': u'северный административный округ',\n",
        "u'свао': u'северо-восточный округ',\n",
        "u'сзао': u'северо-западный округ',\n",
        "u'оуфмс': u'отдел управление федеральной миграционной службы',\n",
        "u'офмс': u'отдел федеральной миграционной службы',\n",
        "u'уфмс': u'управление федеральной миграционной службы',\n",
        "u'увд': u'управление внутренних дел',\n",
        "u'ровд': u'районный отдел внутренних дел',\n",
        "u'говд': u'городской отдел внутренних дел',\n",
        "u'рувд': u'районное управление внутренних дел',\n",
        "u'овд': u'отдел внутренних дел',\n",
        "u'оувд': u'отдел управления внутренних дел',\n",
        "u'мро': u'межрайонный отдел',\n",
        "u'юао': u'южный',\n",
        "u'ювао': u'юго-восточный',\n",
        "u'пс': u'паспортный стол',\n",
        "u'тп': u'территориальный пункт'}\n",
        "## - ВАШ КОД ТУТ - ##}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClQoMFoJEDNH"
      },
      "source": [
        "Теперь, собственно произведем расшифровку абривеатур и отформатируем полученные записи:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sfBDi23IDnq"
      },
      "source": [
        "for i in sokr.keys():\n",
        "    train.passport_issuer_name = train.passport_issuer_name.str.replace(u'( %s )|(^%s)|(%s$)' % (i,i,i), u' %s ' % (sokr[i]))\n",
        "    \n",
        "#удалим лишние пробелы в конце и начале строки\n",
        "train.passport_issuer_name = train.passport_issuer_name.str.lstrip()\n",
        "train.passport_issuer_name = train.passport_issuer_name.str.rstrip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3RRZDNOJ-k2"
      },
      "source": [
        "Нам нужно найти такое k, начиная с которого значение критерия k-means будет убывать не слишком быстро. Этот эффект очень визуально похож на локоть и отсюда, собственно, название этого метода (Метод Локтя). Например, для данных на рисунке выше, таким k будет k равное 4. Важно понимать, что все эти эвристики и меры качества в кластеризации носят лишь рекомендательный характер."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NND1I316jnmL"
      },
      "source": [
        "####Столбец - Дата выдачи:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPgyc2MPjhs8"
      },
      "source": [
        "Предварительный этап обработки поля «кем выдан паспорт» на этом закончим. И перейдем к полю, в котором находится дата выдачи.\n",
        "Как можно заметить данные в нем хранятся в виде: месяцMгод.\n",
        "Соответственно можно просто убрать букву «M» и привести поле к числовому типу. Но если хорошо подумать, то это поле можно удалить, т.к. на один месяц в году может приходиться несколько подразделений выдававших паспорт, и соответственно это может испортить нашу модель. Исходя из этого удалим его из выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiQrmYAylg1p"
      },
      "source": [
        "train = train.drop(['passport_issue_month/year'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDPczr9jmI-y"
      },
      "source": [
        "Теперь мы можем перейти к анализу данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJKPDhW6mhkP"
      },
      "source": [
        "##Анализ данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4TK23v3mocr"
      },
      "source": [
        "Итак, данные для построения модели у нас есть, но они находятся в текстовом виде. Для построения модели хорошо бы было их закодировать в числовом виде.\n",
        "Авторы пакета scikit-learn заботливо о нас позаботились и добавили несколько способов для извлечения и кодирования текстовых данных.\n",
        "\n",
        "* [FeatureHasher](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.FeatureHasher.html#sklearn.feature_extraction.FeatureHasher)\n",
        "\n",
        "* [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
        "\n",
        "* [HashingVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ66O07qmom0"
      },
      "source": [
        "**FeatureHasher** преобразовывает строку в числовой массив заданной длинной с помощью хэш-функции (32-разрядная версия Murmurhash3)\n",
        "\n",
        "**CountVectorizer** преобразовывает входной текст в матрицу, значениями которой, являются количества вхождения данного ключа(слова) в текст. В отличие от FeatureHasher имеет больше настраиваемых параметров(например можно задать токенизатор), но работает медленнее.\n",
        "Для более точного понимания работы CountVectorizer приведем простой пример. Допустим есть таблица с текстовыми значениями:\n",
        "\n",
        "раз два три\n",
        "\n",
        "три четыре два два\n",
        "\n",
        "раз раз раз четыре\n",
        "\n",
        "\n",
        "Для начала CountVectorizer собирает уникальные ключи из всех записей, в нашем примере это будет:\n",
        "\n",
        "[раз, два, три, четыре]\n",
        "\n",
        "Длина списка из уникальных ключей и будет длиной нашего закодированного текста (в нашем случае это 4). А номера элементов будут соответствовать, количеству раз встречи данного ключа с данным номером в строке:\n",
        "\n",
        "раз два три --> [1,1,1,0]\n",
        "три четыре два два --> [0,2,1,1]\n",
        "\n",
        "Соответственно после кодировки, применения данного метода мы получим:\n",
        "\n",
        "1,1,1,0\n",
        "\n",
        "0,2,1,1\n",
        "\n",
        "3,0,0,1\n",
        "\n",
        "\n",
        "**HashingVectorizer** является смесью двух выше описанных методов. В нем можно и регулировать размер закодированной строки (как в FeatureHasher) и настраивать токенизатор (как в CountVectorizer). К тому же его производительность ближе к FeatureHasher.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFIaa7C4movp"
      },
      "source": [
        "\n",
        "Итак, вернемся к анализу. Если мы посмотрим по внимательнее на наш набор данных то можно заметить, что есть похожие строки но записанные по разному например: \"… республика карелия...\" и \"… по республике карелия...\".\n",
        "\n",
        "Для этой задачи хорошо подходит pymorphy или nltk. Мы будем использовать первый, т.к. он изначально создавался для работы с русским языком. Итак, функция которая будет отвечать за нормализацию и очиску строки выглядит так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pmOi4RhQfW_"
      },
      "source": [
        "def f_tokenizer(s):\n",
        "    path=\"/usr/local/lib/python3.6/dist-packages/pymorphy2_dicts_ru/data\"\n",
        "    morph = pymorphy2.MorphAnalyzer(path=path, lang='ru')\n",
        "    if isinstance(s, str):\n",
        "        t = s.split(' ')\n",
        "    else:\n",
        "        t = s\n",
        "    f = []\n",
        "    for j in t:\n",
        "        m = morph.parse(j.replace('.',''))\n",
        "        if len(m) != 0:\n",
        "            wrd = m[0]\n",
        "            if wrd.tag.POS not in ('NUMR','PREP','CONJ','PRCL','INTJ'):\n",
        "                f.append(wrd.normal_form)\n",
        "    return f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZb8n71x5-6Y"
      },
      "source": [
        "Функция делает следующее:\n",
        "* Сначала она преобразовывает строку в список\n",
        "* Затем для всех слов производит разбор\n",
        "* Если слово является числительным, предикативном, предлогом, союзом, частицей или междометием не включаем его в конечный набор\n",
        "* Если слово не попало в предыдущий список, берем его нормальную форму и добавляем в финальный набор\n",
        "\n",
        "Теперь, когда есть функция для нормализации можно приступить к кодированию с помощью метода CountVectorizer. Он выбран потому, что ему можно передать нашу функцию, как токенизатор и он составит список ключей по значениям полученным в результате работы нашей функции:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOAjjat39rN8"
      },
      "source": [
        "coder = HashingVectorizer(tokenizer=f_tokenizer, n_features=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxAXonYjTYNe"
      },
      "source": [
        "Как можно заметить при создании метода кроме токенизатора мы задаем еще один параметр n_features. Через данный параметр задается длина закодированной строки (в нашем случае строка кодируется при помощи 256 столбцов). Кроме того, у HashingVectorizer есть еще одно преимущество перед CountVectorizer, но сразу может выполнять нормализацию значений, что хорошо для таких алгоритмов, как SVM.\n",
        "Теперь применим наш кодировщик к обучающему набору:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whMaNNF8VHbl"
      },
      "source": [
        "TrainNotDuble = train.iloc[1:50000].drop_duplicates() \n",
        "# тут мы берем значение от 1 до 10000. Выполняться код в таком случае будет 8:30 минут. \n",
        "# Можете взять больше или меньше - ждать придется соответственно, но и работа функции изменится!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7XzmhxiTh2H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "507a6ae9-49af-4711-bcbf-7edbb89fb536"
      },
      "source": [
        "trn = coder.fit_transform(TrainNotDuble.passport_issuer_name.tolist()).toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-155-f900529a4e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainNotDuble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpassport_issuer_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0mDocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \"\"\"\n\u001b[0;32m--> 809\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_hasher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0manalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_hasher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/_hash.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_X)\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             _hashing_transform(raw_X, self.n_features, self.dtype,\n\u001b[0;32m--> 155\u001b[0;31m                                self.alternate_sign, seed=0)\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msklearn/feature_extraction/_hashing_fast.pyx\u001b[0m in \u001b[0;36msklearn.feature_extraction._hashing_fast.transform\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/_hash.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mraw_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_iteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"string\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mraw_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             _hashing_transform(raw_X, self.n_features, self.dtype,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0manalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_hasher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-146-5ed98290420b>\u001b[0m in \u001b[0;36mf_tokenizer\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/usr/local/lib/python3.6/dist-packages/pymorphy2_dicts_ru/data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmorph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpymorphy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMorphAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ru'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, lang, result_type, units, probability_estimator_cls, char_substitutes)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopencorpora_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymorphy2/opencorpora_dict/wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading dictionaries from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format: %(format_version)s, revision: %(source_revision)s, updated: %(compiled_at)s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymorphy2/opencorpora_dict/storage.py\u001b[0m in \u001b[0;36mload_dict\u001b[0;34m(path, gramtab_format)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mstr_gramtab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_gramtab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgramtab_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mgramtab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr_gramtab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0msuffixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'suffixes.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymorphy2/opencorpora_dict/storage.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mstr_gramtab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_gramtab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgramtab_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mgramtab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr_gramtab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0msuffixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'suffixes.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymorphy2/tagset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;31m# - grammemes are interned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mgrammemes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mgrammemes_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mintern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrammemes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_grammemes_are_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammemes_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pymorphy2/tagset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;31m# - grammemes are interned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mgrammemes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mgrammemes_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mintern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrammemes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_grammemes_are_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammemes_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z4JvCfgLH4B"
      },
      "source": [
        "#Построение модели\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgb-vSD-L5qx"
      },
      "source": [
        "Для начала нам надо задать значения для столбца, в котором будут содержаться метки классов:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6XbK-EQL2gZ"
      },
      "source": [
        "target = TrainNotDuble.passport_div_code.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdpeQ7OHMlWz"
      },
      "source": [
        "Задача, которую мы решаем сегодня, принадлежит к классу задач классификации со множеством классов. Для решения данной задачи лучше всего подошел алгоритм RandomForest. Остальные алгоритмы показали очень плохие результаты (менее 50%) поэтому я решил не занимать место в статье. При желании любой интересующийся может проверить данные результаты.\n",
        "Для оценки качества классификации будем использовать количество документов по которым принято правильное решение, т. е.\n",
        "\n",
        "Accuracy = {P} / {N}\n",
        "\n",
        ", где P — количество документов по которым классификатор принял правильное решение, а N – размер обучающей выборки.\n",
        "В пакете scikit-learn для этого есть функция: accuracy_score\n",
        "Перед началом построения собственно модели, давайте сократим размерность с помощью «метода главных компонент», т.к. 256 столбцов для обучения довольно много:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCeZJjVvMiqE"
      },
      "source": [
        "pca = PCA(n_components = 15)\n",
        "trn = pca.fit_transform(trn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfkD2XL2NnG1"
      },
      "source": [
        "Модель будет выглядеть так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcur3NAKNelZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b42f97fe-4f01-4dd4-fd84-12c96f60c4f2"
      },
      "source": [
        "model = RandomForestClassifier(n_estimators = 100, criterion='entropy')\n",
        "\n",
        "TRNtrain, TRNtest, TARtrain, TARtest = train_test_split(trn, target, test_size=0.4)\n",
        "model.fit(TRNtrain, TARtrain)\n",
        "print ('accuracy_score: ', accuracy_score(TARtest, model.predict(TRNtest)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score:  0.5209471766848816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Zv_L_U8eGod"
      },
      "source": [
        "#Заключение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJtVJVd3W_Hl"
      },
      "source": [
        "Точность полученная в результате работы - 0.5209471766848816\n",
        ". Эта точность близка к \"угадыванию\". Для улучшения точности можно обрабытывать грамматические ошибки и дргугие опечатки. Обучение тестовой выборки при выполнении работы не производилось так как оно сводилось бы лишь к приведению выборки к нужному виду."
      ]
    }
  ]
}